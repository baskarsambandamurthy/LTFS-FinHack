{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 800)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import gc\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.22 s, sys: 416 ms, total: 2.64 s\n",
      "Wall time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PATH=\"../input/ltfs-fin-model/\"\n",
    "train = pd.read_csv(PATH+\"train_preproc.csv\",index_col=0)\n",
    "test = pd.read_csv(PATH+\"test_preproc.csv\",index_col=0)\n",
    "# subm_df = pd.read_csv(PATH+\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((233154, 52), (112392, 52))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetcol = 'loan_default'\n",
    "target = train[targetcol]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AVERAGE.ACCT.AGE</th>\n",
       "      <th>Aadhar_flag</th>\n",
       "      <th>CREDIT.HISTORY.LENGTH</th>\n",
       "      <th>Current_pincode_ID</th>\n",
       "      <th>DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS</th>\n",
       "      <th>Date.of.Birth</th>\n",
       "      <th>DisbursalDate</th>\n",
       "      <th>Driving_flag</th>\n",
       "      <th>Employee_code_ID</th>\n",
       "      <th>Employment.Type</th>\n",
       "      <th>MobileNo_Avl_Flag</th>\n",
       "      <th>NEW.ACCTS.IN.LAST.SIX.MONTHS</th>\n",
       "      <th>NO.OF_INQUIRIES</th>\n",
       "      <th>PAN_flag</th>\n",
       "      <th>PERFORM_CNS.SCORE</th>\n",
       "      <th>PERFORM_CNS.SCORE.DESCRIPTION</th>\n",
       "      <th>PRI.ACTIVE.ACCTS</th>\n",
       "      <th>PRI.CURRENT.BALANCE</th>\n",
       "      <th>PRI.DISBURSED.AMOUNT</th>\n",
       "      <th>PRI.NO.OF.ACCTS</th>\n",
       "      <th>PRI.OVERDUE.ACCTS</th>\n",
       "      <th>PRI.SANCTIONED.AMOUNT</th>\n",
       "      <th>PRIMARY.INSTAL.AMT</th>\n",
       "      <th>Passport_flag</th>\n",
       "      <th>SEC.ACTIVE.ACCTS</th>\n",
       "      <th>SEC.CURRENT.BALANCE</th>\n",
       "      <th>SEC.DISBURSED.AMOUNT</th>\n",
       "      <th>SEC.INSTAL.AMT</th>\n",
       "      <th>SEC.NO.OF.ACCTS</th>\n",
       "      <th>SEC.OVERDUE.ACCTS</th>\n",
       "      <th>SEC.SANCTIONED.AMOUNT</th>\n",
       "      <th>State_ID</th>\n",
       "      <th>UniqueID</th>\n",
       "      <th>VoterID_flag</th>\n",
       "      <th>asset_cost</th>\n",
       "      <th>branch_id</th>\n",
       "      <th>disbursed_amount</th>\n",
       "      <th>loan_default</th>\n",
       "      <th>ltv</th>\n",
       "      <th>manufacturer_id</th>\n",
       "      <th>supplier_id</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>age</th>\n",
       "      <th>disbursal_date</th>\n",
       "      <th>disbursal_month</th>\n",
       "      <th>disbursal_year</th>\n",
       "      <th>disbursal_day</th>\n",
       "      <th>disbursal_dayofweek</th>\n",
       "      <th>PERFORM_CNS.SCORE.CATEGORY</th>\n",
       "      <th>Employment.Type.Category</th>\n",
       "      <th>AVERAGE.ACCT.AGE_MONTHS</th>\n",
       "      <th>CREDIT.HISTORY.LENGTH_MONTHS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>1</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>1441</td>\n",
       "      <td>0</td>\n",
       "      <td>01-01-84</td>\n",
       "      <td>03-08-18</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Bureau History Available</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>420825</td>\n",
       "      <td>0</td>\n",
       "      <td>58400</td>\n",
       "      <td>67</td>\n",
       "      <td>50578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.55</td>\n",
       "      <td>45</td>\n",
       "      <td>22807</td>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>35.279945</td>\n",
       "      <td>2018-08-03</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1yrs 11mon</td>\n",
       "      <td>1</td>\n",
       "      <td>1yrs 11mon</td>\n",
       "      <td>1502</td>\n",
       "      <td>1</td>\n",
       "      <td>31-07-85</td>\n",
       "      <td>26-09-18</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>Self employed</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>598</td>\n",
       "      <td>I-Medium Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>27600</td>\n",
       "      <td>50200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50200</td>\n",
       "      <td>1991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>537409</td>\n",
       "      <td>0</td>\n",
       "      <td>65550</td>\n",
       "      <td>67</td>\n",
       "      <td>47145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.23</td>\n",
       "      <td>45</td>\n",
       "      <td>22807</td>\n",
       "      <td>1985-07-31</td>\n",
       "      <td>33.700205</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>1</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>1497</td>\n",
       "      <td>0</td>\n",
       "      <td>24-08-85</td>\n",
       "      <td>01-08-18</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>Self employed</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Bureau History Available</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>417566</td>\n",
       "      <td>0</td>\n",
       "      <td>61360</td>\n",
       "      <td>67</td>\n",
       "      <td>53278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.63</td>\n",
       "      <td>45</td>\n",
       "      <td>22807</td>\n",
       "      <td>1985-08-24</td>\n",
       "      <td>33.634497</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0yrs 8mon</td>\n",
       "      <td>1</td>\n",
       "      <td>1yrs 3mon</td>\n",
       "      <td>1501</td>\n",
       "      <td>0</td>\n",
       "      <td>30-12-93</td>\n",
       "      <td>26-10-18</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>Self employed</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>L-Very High Risk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>624493</td>\n",
       "      <td>0</td>\n",
       "      <td>66113</td>\n",
       "      <td>67</td>\n",
       "      <td>57513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.48</td>\n",
       "      <td>45</td>\n",
       "      <td>22807</td>\n",
       "      <td>1993-12-30</td>\n",
       "      <td>25.284052</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>10</td>\n",
       "      <td>2018</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>1</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>1495</td>\n",
       "      <td>0</td>\n",
       "      <td>09-12-77</td>\n",
       "      <td>26-09-18</td>\n",
       "      <td>0</td>\n",
       "      <td>1998</td>\n",
       "      <td>Self employed</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Bureau History Available</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>539055</td>\n",
       "      <td>0</td>\n",
       "      <td>60300</td>\n",
       "      <td>67</td>\n",
       "      <td>52378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.39</td>\n",
       "      <td>45</td>\n",
       "      <td>22807</td>\n",
       "      <td>1977-12-09</td>\n",
       "      <td>41.341547</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>9</td>\n",
       "      <td>2018</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AVERAGE.ACCT.AGE  Aadhar_flag CREDIT.HISTORY.LENGTH  Current_pincode_ID  \\\n",
       "0        0yrs 0mon            1             0yrs 0mon                1441   \n",
       "1       1yrs 11mon            1            1yrs 11mon                1502   \n",
       "2        0yrs 0mon            1             0yrs 0mon                1497   \n",
       "3        0yrs 8mon            1             1yrs 3mon                1501   \n",
       "4        0yrs 0mon            1             0yrs 0mon                1495   \n",
       "\n",
       "   DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS Date.of.Birth DisbursalDate  \\\n",
       "0                                    0      01-01-84      03-08-18   \n",
       "1                                    1      31-07-85      26-09-18   \n",
       "2                                    0      24-08-85      01-08-18   \n",
       "3                                    0      30-12-93      26-10-18   \n",
       "4                                    0      09-12-77      26-09-18   \n",
       "\n",
       "   Driving_flag  Employee_code_ID Employment.Type  MobileNo_Avl_Flag  \\\n",
       "0             0              1998        Salaried                  1   \n",
       "1             0              1998   Self employed                  1   \n",
       "2             0              1998   Self employed                  1   \n",
       "3             0              1998   Self employed                  1   \n",
       "4             0              1998   Self employed                  1   \n",
       "\n",
       "   NEW.ACCTS.IN.LAST.SIX.MONTHS  NO.OF_INQUIRIES  PAN_flag  PERFORM_CNS.SCORE  \\\n",
       "0                             0                0         0                  0   \n",
       "1                             0                0         0                598   \n",
       "2                             0                0         0                  0   \n",
       "3                             0                1         0                305   \n",
       "4                             0                1         0                  0   \n",
       "\n",
       "  PERFORM_CNS.SCORE.DESCRIPTION  PRI.ACTIVE.ACCTS  PRI.CURRENT.BALANCE  \\\n",
       "0   No Bureau History Available                 0                    0   \n",
       "1                 I-Medium Risk                 1                27600   \n",
       "2   No Bureau History Available                 0                    0   \n",
       "3              L-Very High Risk                 0                    0   \n",
       "4   No Bureau History Available                 0                    0   \n",
       "\n",
       "   PRI.DISBURSED.AMOUNT  PRI.NO.OF.ACCTS  PRI.OVERDUE.ACCTS  \\\n",
       "0                     0                0                  0   \n",
       "1                 50200                1                  1   \n",
       "2                     0                0                  0   \n",
       "3                     0                3                  0   \n",
       "4                     0                0                  0   \n",
       "\n",
       "   PRI.SANCTIONED.AMOUNT  PRIMARY.INSTAL.AMT  Passport_flag  SEC.ACTIVE.ACCTS  \\\n",
       "0                      0                   0              0                 0   \n",
       "1                  50200                1991              0                 0   \n",
       "2                      0                   0              0                 0   \n",
       "3                      0                  31              0                 0   \n",
       "4                      0                   0              0                 0   \n",
       "\n",
       "   SEC.CURRENT.BALANCE  SEC.DISBURSED.AMOUNT  SEC.INSTAL.AMT  SEC.NO.OF.ACCTS  \\\n",
       "0                    0                     0               0                0   \n",
       "1                    0                     0               0                0   \n",
       "2                    0                     0               0                0   \n",
       "3                    0                     0               0                0   \n",
       "4                    0                     0               0                0   \n",
       "\n",
       "   SEC.OVERDUE.ACCTS  SEC.SANCTIONED.AMOUNT  State_ID  UniqueID  VoterID_flag  \\\n",
       "0                  0                      0         6    420825             0   \n",
       "1                  0                      0         6    537409             0   \n",
       "2                  0                      0         6    417566             0   \n",
       "3                  0                      0         6    624493             0   \n",
       "4                  0                      0         6    539055             0   \n",
       "\n",
       "   asset_cost  branch_id  disbursed_amount  loan_default    ltv  \\\n",
       "0       58400         67             50578           0.0  89.55   \n",
       "1       65550         67             47145           1.0  73.23   \n",
       "2       61360         67             53278           0.0  89.63   \n",
       "3       66113         67             57513           1.0  88.48   \n",
       "4       60300         67             52378           1.0  88.39   \n",
       "\n",
       "   manufacturer_id  supplier_id date_of_birth        age disbursal_date  \\\n",
       "0               45        22807    1984-01-01  35.279945     2018-08-03   \n",
       "1               45        22807    1985-07-31  33.700205     2018-09-26   \n",
       "2               45        22807    1985-08-24  33.634497     2018-08-01   \n",
       "3               45        22807    1993-12-30  25.284052     2018-10-26   \n",
       "4               45        22807    1977-12-09  41.341547     2018-09-26   \n",
       "\n",
       "   disbursal_month  disbursal_year  disbursal_day  disbursal_dayofweek  \\\n",
       "0                8            2018              3                    4   \n",
       "1                9            2018             26                    2   \n",
       "2                8            2018              1                    2   \n",
       "3               10            2018             26                    4   \n",
       "4                9            2018             26                    2   \n",
       "\n",
       "   PERFORM_CNS.SCORE.CATEGORY  Employment.Type.Category  \\\n",
       "0                           0                         0   \n",
       "1                           1                         1   \n",
       "2                           0                         1   \n",
       "3                           2                         1   \n",
       "4                           0                         1   \n",
       "\n",
       "   AVERAGE.ACCT.AGE_MONTHS  CREDIT.HISTORY.LENGTH_MONTHS  \n",
       "0                        0                             0  \n",
       "1                       23                            23  \n",
       "2                        0                             0  \n",
       "3                        8                            15  \n",
       "4                        0                             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getenccolname(colname,cols_agg):\n",
    "    if 'var' in cols_agg:\n",
    "        colname =\"targetvarenc_\"+colname\n",
    "    elif 'std' in cols_agg:\n",
    "        colname =\"targetstdenc_\"+colname \n",
    "    elif 'sum' in cols_agg:   \n",
    "        colname =\"targetsumenc_\"+colname\n",
    "    elif 'min' in cols_agg:   \n",
    "        colname =\"targetminenc_\"+colname\n",
    "    elif 'max' in cols_agg:   \n",
    "        colname =\"targetmaxenc_\"+colname\n",
    "    elif 'median' in cols_agg:   \n",
    "        colname =\"targetmedianenc_\"+colname\n",
    "    elif 'count' in cols_agg:   \n",
    "        colname =\"targetcountenc_\"+colname\n",
    "    elif 'iqmean' in cols_agg:   \n",
    "        colname =\"targetiqmeanenc_\"+colname\n",
    "    else:\n",
    "        colname =\"targetenc_\"+colname\n",
    "        \n",
    "    return colname\n",
    "\n",
    "def droptargetenccols(train, val,test):\n",
    "     #remove target encoding fields if present\n",
    "    targetenccols = [col for col in train.columns if ('targetenc' in col) or ('targetstdenc' in col)]\n",
    "    train.drop(targetenccols,axis=1,inplace=True)\n",
    "    val.drop(targetenccols,axis=1,inplace=True)\n",
    "    targetenccols_test = [col for col in test.columns if ('targetenc' in col) or ('targetstdenc' in col)]\n",
    "    test.drop(targetenccols_test,axis=1,inplace=True)\n",
    "    \n",
    "    return train, val,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impact_coding(data, feature, target, n_folds=20, n_inner_folds=10):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    '''\n",
    "    ! Using oof_default_mean for encoding inner folds introduces leak.\n",
    "    \n",
    "    Source: https://www.kaggle.com/tnarik/likelihood-encoding-of-categorical-features\n",
    "    \n",
    "    Changelog:    \n",
    "    a) Replaced KFold with StratifiedFold due to class imbalance\n",
    "    b) Rewrote .apply() with .map() for readability\n",
    "    c) Removed redundant apply in the inner loop\n",
    "    d) Removed global average; use local mean to fill NaN values in out-of-fold set\n",
    "    '''\n",
    "    impact_coded = pd.Series()\n",
    "        \n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=8888) # KFold in the original\n",
    "    oof_mean_cv = pd.DataFrame()\n",
    "    split = 0\n",
    "    print('col:',feature)\n",
    "    print()\n",
    "    for outer_, (infold, oof) in enumerate(kf.split(data[feature], data[target])):\n",
    "        print('outer fold:{0} '.format(outer_))\n",
    "\n",
    "        kf_inner = StratifiedKFold(n_splits=n_inner_folds, shuffle=True, random_state=8888)\n",
    "        inner_split = 0\n",
    "        inner_oof_mean_cv = pd.DataFrame()\n",
    "        oof_default_inner_mean = data.iloc[infold][target].mean()\n",
    "        \n",
    "        tr_outer = data.iloc[infold]\n",
    "        \n",
    "        for inner_,(infold_inner, oof_inner) in enumerate(kf_inner.split(data.iloc[infold], data.loc[infold, target])):\n",
    "                    \n",
    "            # The mean to apply to the inner oof split (a 1/n_folds % based on the rest)\n",
    "#             oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n",
    "            oof_mean = tr_outer.iloc[infold_inner].groupby(by=feature)[target].mean()\n",
    "            \n",
    "            # Also populate mapping (this has all group -> mean for all inner CV folds)\n",
    "            inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n",
    "            inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n",
    "            inner_split += 1\n",
    "\n",
    "        # compute mean for each value of categorical value across oof iterations\n",
    "        inner_oof_mean_cv_map = inner_oof_mean_cv.mean(axis=1)\n",
    "\n",
    "        # Also populate mapping\n",
    "        oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n",
    "        oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True) # <- local mean as default\n",
    "        split += 1\n",
    "\n",
    "        feature_mean = data.loc[oof, feature].map(inner_oof_mean_cv_map).fillna(oof_default_inner_mean)\n",
    "        impact_coded = impact_coded.append(feature_mean)\n",
    "    \n",
    "    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n",
    "    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n",
    "\n",
    "\n",
    "\n",
    "def encode_target_cv(tr, val, test, targetcolname, categ_variables, impact_coder=impact_coding,\n",
    "                    n_folds=20, n_inner_folds=10):\n",
    "    \"\"\"Apply original function for each <categ_variables> in  <data>\n",
    "    Reduced number of validation folds\n",
    "    \"\"\"\n",
    "    train_target = tr.copy() \n",
    "    \n",
    "    code_map = dict()\n",
    "    default_map = dict()\n",
    "    for f in categ_variables:\n",
    "        enccol_mean = getenccolname(f,'mean')\n",
    "        train_target.loc[:, enccol_mean], code_map[f], default_map[f] = impact_coder(train_target, f, \n",
    "                                                                                     targetcolname,\n",
    "                                                                                    n_folds=n_folds, \n",
    "                                                                                     n_inner_folds=n_inner_folds)\n",
    "        val.loc[:, enccol_mean] = val[f].map(code_map[f]).fillna(default_map[f])\n",
    "        test.loc[:, enccol_mean] = test[f].map(code_map[f]).fillna(default_map[f])\n",
    "        \n",
    "        \n",
    "#     return train_target, code_map, default_map\n",
    "    return train_target,val,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performsmoothing(averages,targetcolname,train,agg,countSeries,\n",
    "                     smoothing,min_samples_leaf,noise_level,global_agg_val=None):\n",
    "    \n",
    "#         smoothing_v = 1 / (1 + np.exp(-((averages[\"count\"] ) - min_samples_leaf) / smoothing))\n",
    "    smoothing_v = 1 / (1 + np.exp(-((countSeries - min_samples_leaf) / smoothing)) )\n",
    "\n",
    "#         print('averages[count] describe:',averages[\"count\"].describe())\n",
    "    print('smoothing_v describe:',smoothing_v.describe())\n",
    "    if agg=='mean':\n",
    "#             global_agg_val = train[targetcolname].mean()\n",
    "#             print('train[targetcolname] describe:',train[targetcolname].describe())\n",
    "        if global_agg_val is None:\n",
    "            global_agg_val = np.nanmean(train[targetcolname].values)\n",
    "    elif agg=='std':\n",
    "        global_agg_val= train[targetcolname].std()\n",
    "\n",
    "    newcol ='newcol'\n",
    "    if agg=='std':\n",
    "        print('std before smoothing:',averages[agg].head(25))\n",
    "\n",
    "    print('averages[agg].shape:',averages[agg].shape)\n",
    "    print('smoothing_v.shape:',smoothing_v.shape)\n",
    "#         print('global_agg_val:',global_agg_val)\n",
    "\n",
    "    averages[newcol] = global_agg_val * (1 - smoothing_v) + averages[agg] * smoothing_v\n",
    "    \n",
    "#     averages[newcol] = (countSeries * averages[agg] + smoothing * global_agg_val) / (countSeries + smoothing)\n",
    "#         smooth = (counts * means + m * mean) / (counts + m)\n",
    "\n",
    "\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.randn(len(averages[newcol])) * noise_level\n",
    "    averages[newcol] = averages[newcol] + noise\n",
    "\n",
    "    if agg=='std':\n",
    "        print('std after smoothing:',averages[newcol].head(25))\n",
    "\n",
    "    del smoothing_v,noise;gc.collect()\n",
    "\n",
    "    return averages[newcol]\n",
    "\n",
    " \n",
    "def targetencode(train,val,test,catcolnames,targetcolname,\n",
    "                         smoothing,min_samples_leaf,noise_level):\n",
    "    #Target Encoding\n",
    "    \n",
    "    for i,curcol in enumerate(catcolnames):\n",
    "        print()\n",
    "        print('curcol: ',curcol)\n",
    "        print()\n",
    "        \n",
    "        enccol_mean = getenccolname(curcol,'mean')\n",
    "        enccol_count = getenccolname(curcol,'count')\n",
    "        \n",
    "#         print('enccol_mean:',enccol_mean)\n",
    "#         print('enccol_count:',enccol_count)\n",
    "        \n",
    "#         train['group_count'] = train.groupby([curcol]).transform('count')\n",
    "        averages = train.groupby(curcol).agg({targetcolname: \"mean\",\n",
    "                                              curcol: \"count\"})\n",
    "#         averages = train.groupby(curcol).agg({targetcolname: \"mean\",\n",
    "#                                               'group_count': \"first\"})\n",
    "\n",
    "        averages.columns =['mean',enccol_count]\n",
    "#         print('averages count quantile 0.95:',averages[enccol_count].quantile(0.05))\n",
    "#         print('averages count describe:',averages[enccol_count].describe())\n",
    "#         print('averages count quantile 0.95:',averages[enccol_count].quantile(0.95))\n",
    "        print()\n",
    "        print('averages mean quantile 0.95:',averages['mean'].quantile(0.05))\n",
    "        print('averages mean describe:',averages['mean'].describe())\n",
    "        print('averages mean quantile 0.95:',averages['mean'].quantile(0.95))\n",
    "        start = time.time()\n",
    "    \n",
    "        averages[enccol_mean] = performsmoothing(averages,targetcolname,train,'mean',averages[enccol_count],\n",
    "                                          smoothing[i],min_samples_leaf[i],noise_level[i],\n",
    "                                          global_agg_val=None)\n",
    "        averages.drop(['mean'],axis=1,inplace=True)\n",
    "        \n",
    "        train[enccol_mean] = train[curcol].map(averages[enccol_mean])\n",
    "        val[enccol_mean] = val[curcol].map(averages[enccol_mean])\n",
    "        test[enccol_mean] = test[curcol].map(averages[enccol_mean])\n",
    "        \n",
    "#         del train['group_count'];gc.collect()\n",
    "        \n",
    "        end = time.time()\n",
    "        print('update exec time:',end- start)\n",
    "        \n",
    "        del averages;gc.collect()\n",
    "#         print(train[train[enccol_mean].isnull()].shape)\n",
    "        print('val null:',val[val[enccol_mean].isnull()].shape)\n",
    "        print('test null:',test[test[enccol_mean].isnull()].shape)\n",
    "#         print('test unique count:',test.loc[test[enccol_mean].isnull(),curcol].nunique())\n",
    "#         test_missing = test.loc[test[enccol_mean].isnull(),curcol].unique()\n",
    "#         print('test unique null:',test_missing)\n",
    "        \n",
    "#         mask = val[curcol].isin(test_missing)\n",
    "#         val_temp = val[mask]\n",
    "#         print('val of test missing count;',val_temp[curcol].nunique())\n",
    "#         print('val test missing value counts:',val_temp[curcol].value_counts())\n",
    "        \n",
    "#         global_mean= train[enccol_mean].mean()\n",
    "#         train[enccol_mean].fillna(global_mean,inplace=True)\n",
    "#         val[enccol_mean].fillna(global_mean,inplace=True)\n",
    "#         test[enccol_mean].fillna(global_mean,inplace=True)\n",
    "\n",
    "         \n",
    "    return train,val,test\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def runlgb(ispermutefeats,train,test,param,cur_features,score_function=None):\n",
    "\n",
    "    overall_sel_feats =[]\n",
    "    overall_imp_df = pd.DataFrame()\n",
    "    overall_imp_df['feature']= np.array(cur_features)\n",
    "    overall_imp_df['overall_score_mean'] =0 \n",
    "    overall_imp_df['overall_score_max'] =-9999 \n",
    "    overall_imp_df['overall_score_min'] =9999 \n",
    "    \n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    start = time.time()\n",
    "    valid_scores =[]\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=4590)\n",
    "    indices = folds.split(train.values, target.values)\n",
    "        \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(indices):\n",
    "        print()\n",
    "        print(\"fold n°{}\".format(fold_))\n",
    "\n",
    "        tr = train.iloc[trn_idx]\n",
    "        val = train.iloc[val_idx]\n",
    "        y_val = target.iloc[val_idx]\n",
    "        y_tr = target.iloc[trn_idx]\n",
    "        \n",
    "        #Concat current encoding train, valid and test files\n",
    "        cur_tr_encs = tr_encs[fold_]\n",
    "        cur_val_encs= val_encs[fold_]\n",
    "\n",
    "        print('val shape bef:',val.shape)\n",
    "        print('tr shape bef:',tr.shape)\n",
    "        print('test shape bef:',test.shape)\n",
    "        \n",
    "        tr=pd.concat([tr,cur_tr_encs],axis=1)\n",
    "        val=pd.concat([val,cur_val_encs],axis=1)\n",
    "        test_cur=pd.concat([test,test_encs[fold_]],axis=1)\n",
    "        \n",
    "#         targetenccols = ['targetenc_branch_id','targetenc_manufacturer_id','targetenc_State_ID',\n",
    "#                          'targetenc_Current_pincode_ID','targetenc_Employee_code_ID','targetenc_supplier_id']\n",
    "#         targetenccols = ['targetenc_Current_pincode_ID','targetenc_Employee_code_ID','targetenc_supplier_id']\n",
    "\n",
    "#         tr['targetenc_mean'] = tr[targetenccols].mean(axis=1)\n",
    "#         val['targetenc_mean'] = val[targetenccols].mean(axis=1)        \n",
    "#         test_cur['targetenc_mean'] = test_cur[targetenccols].mean(axis=1)        \n",
    "        \n",
    "        print('val shape after:',val.shape)\n",
    "        print('tr shape after:',tr.shape)\n",
    "        print('test shape after:',test.shape)\n",
    "         \n",
    "#         val_index_ser = pd.Series(np.array(val.index))\n",
    "#         print('val shape:',val.shape)\n",
    "#         print('val index head:',val_index_ser.head(20))\n",
    "#         print('val index tail:',val_index_ser.tail(20))\n",
    "        \n",
    "        trn_data = lgb.Dataset(tr[cur_features], label=y_tr)#,, categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(val[cur_features], label=y_val)#,, categorical_feature=categorical_feats)\n",
    "        \n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [val_data], verbose_eval=500, \n",
    "                        early_stopping_rounds = 300)\n",
    "\n",
    "        #Prediction based on current fold selected features\n",
    "        if ispermutefeats:\n",
    "            \n",
    "            selected_features, importance_df = permutation_feature_selection(clf, val[cur_features], \n",
    "                                                                             y_val,score_function,\n",
    "                                                                             rep=4,max_delta_score=max_delta_score)\n",
    "            overall_sel_feats += [selected_features]\n",
    "            print(selected_features)\n",
    "\n",
    "#             print('overal imp shape:{0} importance_df shape:{1}'.format(overall_imp_df.shape,importance_df.shape))\n",
    "            \n",
    "            overall_imp_df['fold_'+str(fold_)+'score_mean'] = importance_df['delta_score_mean']\n",
    "            overall_imp_df['fold_'+str(fold_)+'score_max'] = importance_df['delta_score_max']\n",
    "            overall_imp_df['fold_'+str(fold_)+'score_min'] = importance_df['delta_score_min']\n",
    "        else:\n",
    "            oof[val_idx] = clf.predict(val[cur_features], num_iteration=clf.best_iteration)\n",
    "\n",
    "            fold_importance_df[\"feature\"] = cur_features\n",
    "            if fold_==0:\n",
    "                fold_importance_df[\"importance\"] =0\n",
    "            fold_importance_df[\"importance\"] += clf.feature_importance() / n_splits\n",
    "            valid_scores+=[clf.best_score['valid_0'][param['metric']]]\n",
    "            predictions += clf.predict(test_cur[cur_features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    if ispermutefeats:\n",
    "        fold_mean_cols = [col for col in overall_imp_df.columns if ('score_mean' in col) and ('fold_' in col) ]\n",
    "        fold_max_cols = [col for col in overall_imp_df.columns if ('score_max' in col) and ('fold_' in col) ]\n",
    "        fold_min_cols = [col for col in overall_imp_df.columns if ('score_min' in col) and ('fold_' in col) ]\n",
    "        overall_imp_df['overall_score_mean'] = overall_imp_df[fold_mean_cols].mean(axis=1)\n",
    "        overall_imp_df['overall_score_max'] = overall_imp_df[fold_max_cols].max(axis=1)\n",
    "        overall_imp_df['overall_score_min'] = overall_imp_df[fold_min_cols].min(axis=1)\n",
    "    else:\n",
    "        print('valid scores:',valid_scores)\n",
    "        print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n",
    "\n",
    "    return fold_importance_df,predictions,oof,overall_imp_df,overall_sel_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_cols =['Date.of.Birth','Employment.Type','DisbursalDate',\n",
    "#                'PERFORM_CNS.SCORE.DESCRIPTION','AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH',\n",
    "#                'MobileNo_Avl_Flag','disbursal_year','disbursal_day','disbursal_dayofweek',\n",
    "#                'date_of_birth', 'disbursal_date',\n",
    "#                'UniqueID',targetcol]\n",
    "# features = [c for c in train.columns if c not in exclude_cols]\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param = {'num_leaves': 31,\n",
    "#          'min_data_in_leaf': 30, \n",
    "#          'objective':'binary',\n",
    "#          'max_depth': -1,\n",
    "#          'learning_rate': 0.01,\n",
    "#          \"min_child_samples\": 20,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\": 0.9,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.9 ,\n",
    "#          \"bagging_seed\": 11,\n",
    "#          \"metric\": 'auc',\n",
    "#          \"lambda_l1\": 0.1,\n",
    "#          \"verbosity\": -1,\n",
    "#          \"nthread\": 4,\n",
    "#          'n_estimators' : 10000,\n",
    "#          \"random_state\": 4590}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=5\n",
    "num_round = 10\n",
    "max_delta_score =0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "catcolnames =[\n",
    "             'branch_id',\n",
    "#     'manufacturer_id',      'State_ID',\n",
    "#              'PERFORM_CNS.SCORE.CATEGORY'\n",
    "             'Current_pincode_ID', 'Employee_code_ID', 'supplier_id',\n",
    "             ]\n",
    "\n",
    "\n",
    "# min_samples_leaf=[2000,2000,2000,2000,2000,2000]\n",
    "# smoothing=[10,10,10,10,10,10]\n",
    "# noise_level=[0.1,0.1,0.1,0.1,0.1,0.1]\n",
    "\n",
    "# min_samples_leaf=[2000,2000,2000]\n",
    "# smoothing=[10,10,10]\n",
    "# noise_level=[0.1,0.1,0.1]\n",
    "\n",
    "# min_samples_leaf=[200,200,200]\n",
    "# smoothing=[20,20,20]\n",
    "# noise_level=[0.1,0.1,0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'colsample_bytree': 0.7196484570790651,\n",
    "   'min_child_samples': 235,\n",
    "   'num_leaves': 36,\n",
    "   'reg_alpha': 0.6474702076362333,\n",
    "   'reg_lambda': 0.021458900986429996,\n",
    "   'subsample': 0.8873887256306612,\n",
    "   'subsample_for_bin': 110000,\n",
    "   'learning_rate': 0.01,\n",
    "   'boosting': 'gbdt',\n",
    "   'bagging_seed': 2018,\n",
    "   'bagging_freq': 2,\n",
    "   'min_data_in_bin': 100,\n",
    "   'n_estimators': 10000,\n",
    "   'objective': 'binary',\n",
    "   'metric': 'auc',\n",
    "   'random_state': 2333,\n",
    "   'max_depth': 15,\n",
    "   'scale_pos_weight': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_targetencode(train,target,test,n_splits,catcolnames,targetcol,\n",
    "                    smoothing,min_samples_leaf,noise_level):\n",
    "\n",
    "    start = time.time()\n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=4590)\n",
    "    indices = folds.split(train.values, target.values)\n",
    "\n",
    "    test_index = test.index\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(indices):\n",
    "        print('******************************************************')\n",
    "        print(\"FOLD  ---  {}\".format(fold_))\n",
    "        print('******************************************************')\n",
    "\n",
    "        tr = train.iloc[trn_idx]\n",
    "        val = train.iloc[val_idx]\n",
    "        \n",
    "        tr_index = tr.index\n",
    "        val_index = val.index\n",
    "        #drop any existing target enc cols\n",
    "        tr,val,test = droptargetenccols(tr,val,test)\n",
    "\n",
    "#         #target encoding on transaction merchant id\n",
    "#         tr,val,test = targetencode(tr, val,test,catcolnames,targetcol,\n",
    "#                                     smoothing,min_samples_leaf,noise_level)\n",
    "\n",
    "        tr,val,test = encode_target_cv(tr.reset_index(drop=True), val.reset_index(drop=True), \n",
    "                                       test.reset_index(drop=True), \n",
    "                                       targetcol, catcolnames,\n",
    "                                       n_folds=20, n_inner_folds=10\n",
    "                                      )\n",
    "\n",
    "        tr.index = tr_index\n",
    "        val.index = val_index\n",
    "        test.index = test_index\n",
    "\n",
    "\n",
    "        enc_cols = [col for col in tr.columns if 'targetenc' in col]\n",
    "        print('enc cols:',enc_cols)\n",
    "        print('save encoding feats...')\n",
    "\n",
    "        #save target encoding features in separate file\n",
    "        tr[enc_cols].to_hdf('train_targetenc_feats'+str(fold_)+'.hdf',key='data')\n",
    "        val[enc_cols].to_hdf('val_targetenc_feats'+str(fold_)+'.hdf',key='data')\n",
    "        test[enc_cols].to_hdf('test_targetenc_feats'+str(fold_)+'.hdf',key='data')\n",
    "\n",
    "        print('save encoding feats in csv...')\n",
    "        tr[enc_cols].to_csv('train_targetenc_feats'+str(fold_)+'.csv')\n",
    "        val[enc_cols].to_csv('val_targetenc_feats'+str(fold_)+'.csv')\n",
    "        test[enc_cols].to_csv('test_targetenc_feats'+str(fold_)+'.csv')\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print('Target Enc Execution Time:',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_targetencode(train,target,test,n_splits,catcolnames,targetcol,\n",
    "#                     smoothing,min_samples_leaf,noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getenc():\n",
    "    tr_encs = []\n",
    "    val_encs = []\n",
    "    test_encs = []\n",
    "    \n",
    "    Path=''\n",
    "\n",
    "    for i in range(0,5):\n",
    "        cur_tr_enc = pd.read_csv(Path+'train_targetenc_feats'+str(i)+'.csv',index_col=0)\n",
    "        cur_val_enc = pd.read_csv(Path+'val_targetenc_feats'+str(i)+'.csv',index_col=0)\n",
    "\n",
    "        tr_encs += [cur_tr_enc]\n",
    "        val_encs +=[ cur_val_enc]\n",
    "\n",
    "        test_encs += [pd.read_csv(Path+'test_targetenc_feats'+str(i)+'.csv',index_col=0)]\n",
    "        print('read complete for:',i)\n",
    "        \n",
    "    return tr_encs,val_encs,test_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tr_encs, val_encs,test_encs = getenc(mask_without_outlier=mask_without_outlier)\n",
    "# tr_encs, val_encs,test_encs = getenc()\n",
    "# print(tr_encs[0].shape)\n",
    "# print(val_encs[0].shape)\n",
    "# print(test_encs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targetenccols = ['targetenc_branch_id','targetenc_manufacturer_id','targetenc_State_ID',\n",
    "#                  'targetenc_Current_pincode_ID','targetenc_Employee_code_ID','targetenc_supplier_id']\n",
    "\n",
    "# train['targetenc_mean'] = train[targetenccols].mean(axis=1)\n",
    "# test['targetenc_mean'] = test[targetenccols].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aadhar_flag', 'Current_pincode_ID', 'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS', 'Driving_flag', 'Employee_code_ID', 'NEW.ACCTS.IN.LAST.SIX.MONTHS', 'NO.OF_INQUIRIES', 'PAN_flag', 'PERFORM_CNS.SCORE', 'PRI.ACTIVE.ACCTS', 'PRI.CURRENT.BALANCE', 'PRI.DISBURSED.AMOUNT', 'PRI.NO.OF.ACCTS', 'PRI.OVERDUE.ACCTS', 'PRI.SANCTIONED.AMOUNT', 'PRIMARY.INSTAL.AMT', 'Passport_flag', 'SEC.ACTIVE.ACCTS', 'SEC.CURRENT.BALANCE', 'SEC.DISBURSED.AMOUNT', 'SEC.INSTAL.AMT', 'SEC.NO.OF.ACCTS', 'SEC.OVERDUE.ACCTS', 'SEC.SANCTIONED.AMOUNT', 'State_ID', 'VoterID_flag', 'asset_cost', 'branch_id', 'disbursed_amount', 'ltv', 'manufacturer_id', 'supplier_id', 'age', 'disbursal_month', 'PERFORM_CNS.SCORE.CATEGORY', 'Employment.Type.Category', 'AVERAGE.ACCT.AGE_MONTHS', 'CREDIT.HISTORY.LENGTH_MONTHS', 'targetenc_Current_pincode_ID', 'targetenc_Employee_code_ID', 'targetenc_supplier_id', 'targetenc_branch_id']\n"
     ]
    }
   ],
   "source": [
    "# sel_enc_cols = [col for col in test.columns if 'targetenc_' in col]\n",
    "# print(sel_enc_cols)\n",
    "sel_enc_cols= ['targetenc_Current_pincode_ID','targetenc_Employee_code_ID','targetenc_supplier_id',\n",
    "              'targetenc_branch_id']\n",
    "# sel_enc_cols= ['targetenc_branch_id']\n",
    "exclude_cols =['Date.of.Birth','Employment.Type','DisbursalDate',\n",
    "               'PERFORM_CNS.SCORE.DESCRIPTION','AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH',\n",
    "               'MobileNo_Avl_Flag','disbursal_year','disbursal_day','disbursal_dayofweek',\n",
    "               'date_of_birth', 'disbursal_date',\n",
    "               'UniqueID',targetcol,\n",
    "#               'branch_id', 'manufacturer_id', 'State_ID', 'Current_pincode_ID', \n",
    "#                'Employee_code_ID', 'supplier_id'\n",
    "              ]\n",
    "features = [c for c in train.columns if c not in exclude_cols]\n",
    "features +=sel_enc_cols\n",
    "# features +=['targetenc_mean']\n",
    "\n",
    "# targetenccols = ['targetenc_branch_id','targetenc_manufacturer_id','targetenc_State_ID',\n",
    "#                  'targetenc_Current_pincode_ID','targetenc_Employee_code_ID','targetenc_supplier_id']\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_encs=None; val_encs=None;test_encs=None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runlgb_with_targetenc(train,test,param,features,\n",
    "                         n_splits,catcolnames,targetcol,\n",
    "                    smoothing,min_samples_leaf,noise_level):\n",
    "    \n",
    "    global tr_encs, val_encs,test_encs\n",
    "\n",
    "    gen_targetencode(train,target,test,n_splits,catcolnames,targetcol,\n",
    "                    smoothing,min_samples_leaf,noise_level)\n",
    "\n",
    "    tr_encs, val_encs,test_encs = getenc()\n",
    "    print(tr_encs[0].shape)\n",
    "    print(val_encs[0].shape)\n",
    "    print(test_encs[0].shape)\n",
    "    \n",
    "    fold_importance_df,predictions,oof,overall_imp_df,overall_sel_feats = \\\n",
    "        runlgb(False,train ,test,param,features,score_function=None)\n",
    "    \n",
    "    return fold_importance_df,predictions,oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# min_samples_leaf=[500,500,500]\n",
    "# smoothing=[50,50,50]\n",
    "# noise_level=[0.1,0.1,0.1]\n",
    "\n",
    "\n",
    "# fold_importance_df,predictions,oof = runlgb_with_targetenc(train,test,param,features,\n",
    "#                                      n_splits,catcolnames,targetcol,\n",
    "#                                 smoothing,min_samples_leaf,noise_level)\n",
    "\n",
    "# sub_df = pd.DataFrame({\"UniqueID\":test[\"UniqueID\"].values})\n",
    "# sub_df[targetcol] = predictions\n",
    "# sub_df.to_csv(\"submission_targetenc_100L_10S.csv\", index=False)\n",
    "# np.save('oof_targetenc_100L_10S.npy',oof)\n",
    "# oof1 = oof.copy()\n",
    "# predictions1 = predictions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# min_samples_leaf=[1000,1000,1000]\n",
    "# smoothing=[100,100,100]\n",
    "# noise_level=[0.1,0.1,0.1]\n",
    "\n",
    "# fold_importance_df,predictions,oof = runlgb_with_targetenc(train,test,param,features,\n",
    "#                                      n_splits,catcolnames,targetcol,\n",
    "#                                 smoothing,min_samples_leaf,noise_level)\n",
    "\n",
    "# sub_df = pd.DataFrame({\"UniqueID\":test[\"UniqueID\"].values})\n",
    "# sub_df[targetcol] = predictions\n",
    "# sub_df.to_csv(\"submission_targetenc_100L_0.5S.csv\", index=False)\n",
    "# np.save('oof_targetenc_100L_0.5S.npy',oof)\n",
    "# oof2 = oof.copy()\n",
    "# predictions2 = predictions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************\n",
      "FOLD  ---  0\n",
      "******************************************************\n",
      "col: branch_id\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: Current_pincode_ID\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: Employee_code_ID\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: supplier_id\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "enc cols: ['targetenc_branch_id', 'targetenc_Current_pincode_ID', 'targetenc_Employee_code_ID', 'targetenc_supplier_id']\n",
      "save encoding feats...\n",
      "save encoding feats in csv...\n",
      "******************************************************\n",
      "FOLD  ---  1\n",
      "******************************************************\n",
      "col: branch_id\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: Current_pincode_ID\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: Employee_code_ID\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: supplier_id\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "enc cols: ['targetenc_branch_id', 'targetenc_Current_pincode_ID', 'targetenc_Employee_code_ID', 'targetenc_supplier_id']\n",
      "save encoding feats...\n",
      "save encoding feats in csv...\n",
      "******************************************************\n",
      "FOLD  ---  2\n",
      "******************************************************\n",
      "col: branch_id\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: Current_pincode_ID\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: Employee_code_ID\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: supplier_id\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "enc cols: ['targetenc_branch_id', 'targetenc_Current_pincode_ID', 'targetenc_Employee_code_ID', 'targetenc_supplier_id']\n",
      "save encoding feats...\n",
      "save encoding feats in csv...\n",
      "******************************************************\n",
      "FOLD  ---  3\n",
      "******************************************************\n",
      "col: branch_id\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: Current_pincode_ID\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: Employee_code_ID\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: supplier_id\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "enc cols: ['targetenc_branch_id', 'targetenc_Current_pincode_ID', 'targetenc_Employee_code_ID', 'targetenc_supplier_id']\n",
      "save encoding feats...\n",
      "save encoding feats in csv...\n",
      "******************************************************\n",
      "FOLD  ---  4\n",
      "******************************************************\n",
      "col: branch_id\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: Current_pincode_ID\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: Employee_code_ID\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "col: supplier_id\n",
      "\n",
      "outer fold:0 \n",
      "outer fold:1 \n",
      "outer fold:2 \n",
      "outer fold:3 \n",
      "outer fold:4 \n",
      "outer fold:5 \n",
      "outer fold:6 \n",
      "outer fold:7 \n",
      "outer fold:8 \n",
      "outer fold:9 \n",
      "outer fold:10 \n",
      "outer fold:11 \n",
      "outer fold:12 \n",
      "outer fold:13 \n",
      "outer fold:14 \n",
      "outer fold:15 \n",
      "outer fold:16 \n",
      "outer fold:17 \n",
      "outer fold:18 \n",
      "outer fold:19 \n",
      "enc cols: ['targetenc_branch_id', 'targetenc_Current_pincode_ID', 'targetenc_Employee_code_ID', 'targetenc_supplier_id']\n",
      "save encoding feats...\n",
      "save encoding feats in csv...\n",
      "Target Enc Execution Time: 463.55293369293213\n",
      "read complete for: 0\n",
      "read complete for: 1\n",
      "read complete for: 2\n",
      "read complete for: 3\n",
      "read complete for: 4\n",
      "(186522, 4)\n",
      "(46632, 4)\n",
      "(112392, 4)\n",
      "\n",
      "fold n°0\n",
      "val shape bef: (46632, 52)\n",
      "tr shape bef: (186522, 52)\n",
      "test shape bef: (112392, 52)\n",
      "val shape after: (46632, 56)\n",
      "tr shape after: (186522, 56)\n",
      "test shape after: (112392, 52)\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[500]\tvalid_0's auc: 0.676246\n",
      "[1000]\tvalid_0's auc: 0.679045\n",
      "[1500]\tvalid_0's auc: 0.679372\n",
      "Early stopping, best iteration is:\n",
      "[1538]\tvalid_0's auc: 0.679444\n",
      "\n",
      "fold n°1\n",
      "val shape bef: (46631, 52)\n",
      "tr shape bef: (186523, 52)\n",
      "test shape bef: (112392, 52)\n",
      "val shape after: (46631, 56)\n",
      "tr shape after: (186523, 56)\n",
      "test shape after: (112392, 52)\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[500]\tvalid_0's auc: 0.672893\n",
      "[1000]\tvalid_0's auc: 0.675015\n",
      "[1500]\tvalid_0's auc: 0.675168\n",
      "Early stopping, best iteration is:\n",
      "[1224]\tvalid_0's auc: 0.675205\n",
      "\n",
      "fold n°2\n",
      "val shape bef: (46631, 52)\n",
      "tr shape bef: (186523, 52)\n",
      "test shape bef: (112392, 52)\n",
      "val shape after: (46631, 56)\n",
      "tr shape after: (186523, 56)\n",
      "test shape after: (112392, 52)\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[500]\tvalid_0's auc: 0.671596\n",
      "[1000]\tvalid_0's auc: 0.674998\n",
      "[1500]\tvalid_0's auc: 0.675544\n",
      "[2000]\tvalid_0's auc: 0.675658\n",
      "Early stopping, best iteration is:\n",
      "[2126]\tvalid_0's auc: 0.675738\n",
      "\n",
      "fold n°3\n",
      "val shape bef: (46630, 52)\n",
      "tr shape bef: (186524, 52)\n",
      "test shape bef: (112392, 52)\n",
      "val shape after: (46630, 56)\n",
      "tr shape after: (186524, 56)\n",
      "test shape after: (112392, 52)\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[500]\tvalid_0's auc: 0.672486\n",
      "[1000]\tvalid_0's auc: 0.675573\n",
      "[1500]\tvalid_0's auc: 0.676185\n",
      "Early stopping, best iteration is:\n",
      "[1582]\tvalid_0's auc: 0.676263\n",
      "\n",
      "fold n°4\n",
      "val shape bef: (46630, 52)\n",
      "tr shape bef: (186524, 52)\n",
      "test shape bef: (112392, 52)\n",
      "val shape after: (46630, 56)\n",
      "tr shape after: (186524, 56)\n",
      "test shape after: (112392, 52)\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[500]\tvalid_0's auc: 0.675926\n",
      "[1000]\tvalid_0's auc: 0.678699\n",
      "Early stopping, best iteration is:\n",
      "[1104]\tvalid_0's auc: 0.678861\n",
      "valid scores: [0.6794440326247962, 0.6752050833727562, 0.6757382430174815, 0.676262992661871, 0.6788612476596739]\n",
      "CV score: 0.67707 \n",
      "CPU times: user 26min 20s, sys: 2min 49s, total: 29min 10s\n",
      "Wall time: 13min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param = {'colsample_bytree': 0.7196484570790651,\n",
    "   'min_child_samples': 235,\n",
    "   'num_leaves': 36,\n",
    "   'reg_alpha': 0.6474702076362333,\n",
    "   'reg_lambda': 0.021458900986429996,\n",
    "   'subsample': 0.8873887256306612,\n",
    "   'subsample_for_bin': 110000,\n",
    "   'learning_rate': 0.01,\n",
    "   'boosting': 'gbdt',\n",
    "   'bagging_seed': 2018,\n",
    "   'bagging_freq': 2,\n",
    "   'min_data_in_bin': 100,\n",
    "   'n_estimators': 10000,\n",
    "   'objective': 'binary',\n",
    "   'metric': 'auc',\n",
    "   'random_state': 2333,\n",
    "   'max_depth': 15,\n",
    "   'scale_pos_weight': 1}\n",
    "\n",
    "min_samples_leaf=[1000,1000,1000]\n",
    "smoothing=[100,100,100]\n",
    "noise_level=[0.1,0.1,0.1]\n",
    "\n",
    "fold_importance_df,predictions,oof = runlgb_with_targetenc(train,test,param,features,\n",
    "                                     n_splits,catcolnames,targetcol,\n",
    "                                smoothing,min_samples_leaf,noise_level)\n",
    "\n",
    "sub_df = pd.DataFrame({\"UniqueID\":test[\"UniqueID\"].values})\n",
    "sub_df[targetcol] = predictions\n",
    "sub_df.to_csv(\"submission_targetenc_300L_without_fillNA.csv\", index=False)\n",
    "np.save('oof_targetenc_300L_without_fillNA.npy',oof)\n",
    "oof3 = oof.copy()\n",
    "predictions3 = predictions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "\n",
    "# param = {'num_leaves': 31,\n",
    "#          'min_data_in_leaf': 30, \n",
    "#          'objective':'binary',\n",
    "#          'max_depth': -1,\n",
    "#          'learning_rate': 0.01,\n",
    "#          \"min_child_samples\": 20,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\": 0.9,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.9 ,\n",
    "#          \"bagging_seed\": 11,\n",
    "#          \"metric\": 'auc',\n",
    "#          \"lambda_l1\": 0.1,\n",
    "#          \"verbosity\": -1,\n",
    "#          \"nthread\": 4,\n",
    "#          'n_estimators' : 10000,\n",
    "#          \"random_state\": 4590}\n",
    "\n",
    "\n",
    "# min_samples_leaf=[300,300,300]\n",
    "# smoothing=[30,30,30]\n",
    "# noise_level=[0.1,0.1,0.1]\n",
    "\n",
    "# fold_importance_df,predictions,oof = runlgb_with_targetenc(train,test,param,features,\n",
    "#                                      n_splits,catcolnames,targetcol,\n",
    "#                                 smoothing,min_samples_leaf,noise_level)\n",
    "\n",
    "# sub_df = pd.DataFrame({\"UniqueID\":test[\"UniqueID\"].values})\n",
    "# sub_df[targetcol] = predictions\n",
    "# sub_df.to_csv(\"submission_targetenc_300L_with_normalparam.csv\", index=False)\n",
    "# np.save('oof_targetenc_300L_with_normalparam.npy',oof)\n",
    "# oof2 = oof.copy()\n",
    "# predictions2 = predictions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# fold_importance_df,predictions,oof,overall_imp_df,overall_sel_feats = \\\n",
    "#         runlgb(False,train ,test,param,features,score_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof1 = oof.copy() #min_samples_leaf=[12,62,36]  smoothing=[1,1,1]  new param\n",
    "# oof2 = oof.copy() # min_samples_leaf=[100,100,100]  smoothing=[10,10,10] old param \n",
    "# oof3 = oof.copy() # min_samples_leaf=[100,100,100]  smoothing=[10,10,10] new param\n",
    "# oof4 = oof.copy() #min_samples_leaf=[200,200,200] smoothing=[5,5,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oof1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b12b8e60f1a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moof1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moof2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moof3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ens AUC:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredictions2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredictions3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'oof1' is not defined"
     ]
    }
   ],
   "source": [
    "oof = oof1 + oof2 + oof3 \n",
    "print('Ens AUC:',roc_auc_score(target,oof))\n",
    "predictions = predictions1 + predictions2 + predictions3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"UniqueID\":test[\"UniqueID\"].values})\n",
    "sub_df[targetcol] = predictions\n",
    "sub_df.to_csv(\"submission_targetenc_ens.csv\", index=False)\n",
    "np.save('oof_targetenc_ens.npy',oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# PATH=\"../input/ltfs-fin/\"\n",
    "# train_orig = pd.read_csv(PATH+\"train.csv\")\n",
    "# test_orig = pd.read_csv(PATH+\"test.csv\")\n",
    "# # subm_df = pd.read_csv(PATH+\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catcolnames =[\n",
    "# #     'branch_id','manufacturer_id','State_ID',\n",
    "#              'Current_pincode_ID', 'Employee_code_ID', 'supplier_id',\n",
    "# #              'PERFORM_CNS.SCORE.CATEGORY'\n",
    "#              ]\n",
    "# #sel_enc_cols\n",
    "# for col in catcolnames:\n",
    "#     print()\n",
    "#     print('col:',col)\n",
    "#     test_unique=test_orig[col].unique() \n",
    "#     train_unique=train_orig[col].unique() \n",
    "#     test_m_train = list(set(test_unique).difference(train_unique))\n",
    "#     print((test_m_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catcolnames =[\n",
    "# #     'branch_id','manufacturer_id','State_ID',\n",
    "#              'Current_pincode_ID', 'Employee_code_ID', 'supplier_id',\n",
    "# #              'PERFORM_CNS.SCORE.CATEGORY'\n",
    "#              ]\n",
    "# #sel_enc_cols\n",
    "# for col in catcolnames:\n",
    "#     print()\n",
    "#     print('col:',col)\n",
    "#     print('*********Train***********')\n",
    "#     df = pd.DataFrame()\n",
    "#     df['train']=train[col].value_counts() \n",
    "#     df['train_freq']=df['train'] / df['train'].sum()\n",
    "#     df['test']=test[col].value_counts()\n",
    "#     df['test_freq']=df['test'] / df['test'].sum()\n",
    "# #     df=train[col].value_counts().reset_index()\n",
    "#     print(df.head(100))\n",
    "#     print(df['train'].describe())\n",
    "#     print(df['train'].quantile(0.95))\n",
    "#     print()\n",
    "#     print(df['test'].describe())\n",
    "#     print(df['test'].quantile(0.95))\n",
    "# #     print(train[col].value_counts())\n",
    "#     print('*********TEST***********')\n",
    "# #     print(test[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test['branch_id'].head(25))\n",
    "# print(test_encs[0]['targetenc_branch_id'].head(25))\n",
    "# print(test_encs[1]['targetenc_branch_id'].head(25))\n",
    "# print(test_encs[2]['targetenc_branch_id'].head(25))\n",
    "# print(test_encs[3]['targetenc_branch_id'].head(25))\n",
    "# print(test_encs[4]['targetenc_branch_id'].head(25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
